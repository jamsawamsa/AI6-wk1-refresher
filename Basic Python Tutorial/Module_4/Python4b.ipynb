{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Data Manipulation and Analysis with Pandas\n",
    "\n",
    "![](http://pandas.pydata.org/_static/pandas_logo.png)\n",
    "[Pandas](http://pandas.pydata.org/) is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. Pandas is free software released under the three-clause BSD license. The name is derived from the term _panel data_, an econometrics term for multidimensional structured data sets.\n",
    "\n",
    "#### Contents\n",
    "*[Continued from part 4a]*\n",
    "\n",
    "* [Analyzing Data](#Analyzing-Data)\n",
    "    * [Groupby](#Groupby) \n",
    "    * [MultiIndices](#MultiIndex)\n",
    "    * [Pivot Tables](#Pivot-Tables) like Excel!\n",
    "* [Example Analysis](#Example-Analysis)\n",
    "* [Presenting Data](#Presenting-Data)\n",
    "* [Summary](#Summary)\n",
    "\n",
    "* [Other Topics](#Other-Topics)\n",
    "    * [Merging DataFrames](#Merging-DataFrames)\n",
    "    * [Apply Method](#Apply-method)\n",
    "    * [Scales](#Scales)\n",
    "    * [Dates Functionality](#Date-Functionality-in-Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Data\n",
    "\n",
    "The rest of this worksheet will use a single example, which contains data for customer counts per date at different store locations each week.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to generate test data\n",
    "def CreateDataSet(Number=1):\n",
    "    Output = []\n",
    "    for i in range(Number):\n",
    "        # Create a weekly (mondays) date range\n",
    "        rng = pd.date_range(start='1/1/2013', end='12/31/2016', freq='W-MON')\n",
    "        \n",
    "        # Create random data\n",
    "        data = np.random.randint(low=25,high=1000,size=len(rng))\n",
    "        \n",
    "        # Status pool\n",
    "        status = [1,2,3]\n",
    "        \n",
    "        # Make a random list of statuses\n",
    "        random_status = [status[np.random.randint(low=0,high=len(status))] for i in range(len(rng))]\n",
    "        \n",
    "        # State pool\n",
    "        location = ['Bangsar','Ampang','Petaling Jaya','Cheras']\n",
    "        \n",
    "        # Make a random list of states \n",
    "        random_location = [location[np.random.randint(low=0,high=len(location))] for i in range(len(rng))]\n",
    "\n",
    "        Output.extend(list(zip(random_location, random_status, data, rng)))\n",
    "        \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CreateDataSet(4)\n",
    "df = pd.DataFrame(data=dataset, columns=['location','Status','CustomerCount','StatusDate'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to save this dataframe into an Excel file, to then bring it back to a dataframe. We simply do this to practice reading and writing to Excel files.  \n",
    "\n",
    "We do not write the index values of the dataframe to the Excel file, since they are not meant to be part of our initial test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to Excel\n",
    "df.to_excel('data/Customers.xlsx', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of file\n",
    "Location = r'data/Customers.xlsx'\n",
    "\n",
    "# Parse a specific sheet - look what we did here!\n",
    "df = pd.read_excel(Location, 0, index_col='StatusDate')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the index of df\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pretend that status == only includes the people who \n",
    "# bought something. \n",
    "mask = df['Status'] == 1\n",
    "df = df[mask]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we may want to graph the data to check for any outliers or inconsistencies in the data. We will be using the ***plot()*** attribute of the dataframe.  \n",
    "\n",
    "As you can see from the graph below it is not very conclusive and is probably a sign that we need to perform some more data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CustomerCount'].plot(figsize=(15,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the data, we begin to realize that there are multiple values for the same location, StatusDate, and Status combination. It is possible that this means the data you are working with is dirty/bad/inaccurate, but we will assume otherwise. We can assume this data set is a subset of a bigger data set and if we simply add the values in the ***CustomerCount*** column per location, StatusDate, and Status we will get the ***Total Customer Count*** per day.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortdf = df[df['location']=='Bangsar'].sort_index(axis=0)\n",
    "sortdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "Our task is now to create a new dataframe that compresses the data so we have daily customer counts per location and StatusDate. We can ignore the Status column since the values in this column are of value *1*. To accomplish this we will use the dataframe's functions ***groupby*** and ***sum()***.  \n",
    "\n",
    "Note that we are using **reset_index** . If we did not, we would not have been able to group by both the location and the StatusDate since the groupby function expects only columns as inputs. The **reset_index** function will bring the index ***StatusDate*** back to a column in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by State and StatusDate\n",
    "Daily = df.reset_index().groupby(['location','StatusDate']).sum()\n",
    "Daily.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***location*** and ***StatusDate*** columns are automatically placed in the index of the ***Daily*** dataframe. You can think of the ***index*** as the primary key of a database table but without the constraint of having unique values. Columns in the index as you will see allow us to easily select, plot, and perform calculations on the data.  \n",
    "\n",
    "Below is how we can delete the ***Status*** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Daily['Status']\n",
    "Daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try another groupby!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the index of the Daily dataframe\n",
    "Daily.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the location index\n",
    "Daily.index.levels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Tables\n",
    "\n",
    "Are easy! and are akin to Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Create the data set again.\n",
    "dataset = CreateDataSet(4)\n",
    "df = pd.DataFrame(data=dataset, columns=['location','Status','CustomerCount','StatusDate'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df, values = 'CustomerCount', \n",
    "               index = ['StatusDate','Status'],\n",
    "               columns = ['location']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df, values = 'CustomerCount', \n",
    "               index = ['StatusDate','location'],\n",
    "               columns = ['Status']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggfunc allows you to calculate some useful statistics aggregated over all entries in the dataframe\n",
    "pd.pivot_table(df, values = 'CustomerCount', index = ['location'], aggfunc=('count','sum','mean','max','min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Design your own pivot tables to extract some meaningful information from the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Analysis\n",
    "\n",
    "Let's now plot the data per location.  \n",
    "\n",
    "As you can see by breaking the graph up by the ***location*** column we have a much clearer picture on how the data looks like. **<font color=\"#ec1c24\">Can you spot any outliers?</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in Daily.index.levels[0]:\n",
    "    print(place)\n",
    "    Daily.loc[place].plot()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that per month the customer count should remain relatively steady. Any data outside a specific range in that month will be removed from the data set. The final result should have smooth graphs with no spikes.  \n",
    "\n",
    "***LocationYearMonth*** - Here we group by location, Year of StatusDate, and Month of StatusDate.  \n",
    "***Daily['Outlier']*** - A boolean (True or False) value letting us know if the value in the CustomerCount column is ouside the acceptable range.  \n",
    "\n",
    "We will be using the attribute ***transform*** instead of ***apply***. The reason is that transform will keep the shape(# of rows and columns) of the dataframe the same and apply will not. By looking at the previous graphs, we can realize they are not resembling a gaussian distribution, this means we cannot use summary statistics like the mean and stDev. We use percentiles instead. Note that we run the risk of eliminating good data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Outliers\n",
    "LocationYearMonth = Daily.groupby([Daily.index.get_level_values(0), Daily.index.get_level_values(1).year, Daily.index.get_level_values(1).month])\n",
    "Daily['Lower'] = LocationYearMonth['CustomerCount'].transform( lambda x: x.quantile(q=.25) - (1.5*x.quantile(q=.75)-x.quantile(q=.25)) )\n",
    "Daily['Upper'] = LocationYearMonth['CustomerCount'].transform( lambda x: x.quantile(q=.75) + (1.5*x.quantile(q=.75)-x.quantile(q=.25)) )\n",
    "Daily['Outlier'] = (Daily['CustomerCount'] < Daily['Lower']) | (Daily['CustomerCount'] > Daily['Upper']) \n",
    "\n",
    "# Remove Outliers\n",
    "Daily = Daily[Daily['Outlier'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe named ***Daily*** will hold customer counts that have been aggregated per day. The original data (df) has multiple records per day.  We are left with a data set that is indexed by both the state and the StatusDate. The Outlier column should be equal to ***False*** signifying that the record is not an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a separate dataframe named ***ALL*** which groups the Daily dataframe by StatusDate. We are essentially getting rid of the ***Location*** column. The ***Max*** column represents the maximum customer count per month. The ***Max*** column is used to smooth out the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all markets\n",
    "\n",
    "# Get the max customer count by Date\n",
    "ALL = pd.DataFrame(Daily['CustomerCount'].groupby(Daily.index.get_level_values(1)).sum())\n",
    "ALL.columns = ['CustomerCount'] # rename column\n",
    "\n",
    "# Group by Year and Month\n",
    "YearMonth = ALL.groupby([lambda x: x.year, lambda x: x.month])\n",
    "\n",
    "# What is the max customer count per Year and Month\n",
    "ALL['Max'] = YearMonth['CustomerCount'].transform(lambda x: x.max())\n",
    "ALL.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the ***ALL*** dataframe above, in the month of Jan 2013, the maximum customer count was 1702. If we had used ***apply***, we would have got a dataframe with (Year and Month) as the index and just the *Max* column with the value of 1702. \n",
    "\n",
    "\n",
    "----------------------------------  \n",
    "There is also an interest to gauge if the current customer counts were reaching certain goals the company had established. The task here is to visually show if the current customer counts are meeting the goals listed below. We will call the goals ***BHAG*** (Big Hairy Annual Goal).  \n",
    "\n",
    "* 12/31/2015 - 1,000 customers  \n",
    "* 12/31/2016 - 2,000 customers  \n",
    "* 12/31/2017 - 3,000 customers  \n",
    "\n",
    "We will be using the **date_range** function to create our dates.  \n",
    "\n",
    "***Definition:*** date_range(start=None, end=None, periods=None, freq='D', tz=None, normalize=False, name=None, closed=None)  \n",
    "***Docstring:*** Return a fixed frequency datetime index, with day (calendar) as the default frequency  \n",
    "\n",
    "By choosing the frequency to be ***A*** or annual we will be able to get the three target dates from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.date_range?\n",
    "\n",
    "# try it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presenting Data  \n",
    "\n",
    "Create individual Graphs for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Graph\n",
    "ALL['Max'].plot(figsize=(10, 5));plt.title('ALL Markets')\n",
    "\n",
    "# Last four Graphs\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n",
    "fig.subplots_adjust(hspace=1.0) ## Create space between plots\n",
    "\n",
    "Daily.loc['Bangsar']['CustomerCount']['2016':].fillna(method='pad').plot(ax=axes[0,0])\n",
    "Daily.loc['Petaling Jaya']['CustomerCount']['2016':].fillna(method='pad').plot(ax=axes[0,1]) \n",
    "Daily.loc['Ampang']['CustomerCount']['2016':].fillna(method='pad').plot(ax=axes[1,0]) \n",
    "Daily.loc['Cheras']['CustomerCount']['2016':].fillna(method='pad').plot(ax=axes[1,1]) \n",
    "\n",
    "# Add titles\n",
    "axes[0,0].set_title('Bangsar')\n",
    "axes[0,1].set_title('Petaling Jaya')\n",
    "axes[1,0].set_title('Ampang')\n",
    "axes[1,1].set_title('Cheras')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Pandas provides a \"batteries-included\" basic data analysis:\n",
    "  - **Loading data:** `read_csv`, `read_table`, `read_sql`, and `read_html`\n",
    "  - **Selection, filtering, and aggregation** (i.e., SQL-type operations): There's a special syntax for `SELECT`ing.  There's the `merge` method for `JOIN`ing.  There's also an easy syntax for what in SQL is a mouthful: Creating a new column whose value is computed from other column -- with the bonus that now the computations can use the full power of Python (though it might be faster if it didn't).\n",
    "  - **\"Pivot table\" style aggregation**: If you're an Excel cognosceti, you may appreciate this.\n",
    "  - **NA handling**: Like R's data frames, there is good support for transforming NA values with default values / averaging tricks / etc.\n",
    "  - **Basic statistics:** e.g. `mean`, `median`, `max`, `min`, and the convenient `describe`.\n",
    "  - **Plugging into more advanced analytics:** Okay, this isn't batteries included.  But still, it plays reasonably with `sklearn`.\n",
    "  - **Visualization:** For instance `plot` and `hist`.\n",
    "  \n",
    "Plugging into more advanced analytics\n",
    "-------\n",
    "Almost any \"advanced analytics\" tool in the Python ecosystem is going to take as input `np.array` type arrays.  You can access the underlying array of a data frame column as\n",
    "\n",
    "        df['column'].values\n",
    "        \n",
    "Many of them take `nd.array` whose underlying data can be accessed by \n",
    "\n",
    "        df.values\n",
    "        \n",
    "directly.  *Most* of the time, they will take `df['column']` and `df` without needing to look at values.\n",
    "\n",
    "This is particularly important if you want to use Pandas with the sklearn library. See this [blog post](http://www.markhneedham.com/blog/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/) for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Topics\n",
    "\n",
    "### Merging Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([{'Name': 'Chris', 'Item Purchased': 'Sponge', 'Cost': 22.50},\n",
    "                   {'Name': 'Kevyn', 'Item Purchased': 'Kitty Litter', 'Cost': 2.50},\n",
    "                   {'Name': 'Filip', 'Item Purchased': 'Spoon', 'Cost': 5.00}],\n",
    "                  index=['Store 1', 'Store 1', 'Store 2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = ['December 1', 'January 1', 'mid-May']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Delivered'] = True\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Feedback'] = ['Positive', None, 'Negative']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = df.reset_index()\n",
    "adf['Date'] = pd.Series({0: 'December 1', 2: 'mid-May'})\n",
    "adf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_df = pd.DataFrame([{'Name': 'Kelly', 'Role': 'Director of HR'},\n",
    "                         {'Name': 'Sally', 'Role': 'Course liasion'},\n",
    "                         {'Name': 'James', 'Role': 'Grader'}])\n",
    "staff_df = staff_df.set_index('Name')\n",
    "\n",
    "student_df = pd.DataFrame([{'Name': 'James', 'School': 'Business'},\n",
    "                           {'Name': 'Mike', 'School': 'Law'},\n",
    "                           {'Name': 'Sally', 'School': 'Engineering'}])\n",
    "student_df = student_df.set_index('Name')\n",
    "staff_df.head()\n",
    "#student_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.merge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.dofactory.com/Images/sql-joins.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(staff_df, student_df, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(staff_df, student_df, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(staff_df, student_df, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(staff_df, student_df, how='right', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "staff_df = staff_df.reset_index()\n",
    "student_df = student_df.reset_index()\n",
    "staff_df\n",
    "student_df\n",
    "pd.merge(staff_df, student_df, how='left', left_on='Name', right_on='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_df = pd.DataFrame([{'Name': 'Kelly', 'Role': 'Director of HR', 'Location': 'State Street'},\n",
    "                         {'Name': 'Sally', 'Role': 'Course liasion', 'Location': 'Washington Avenue'},\n",
    "                         {'Name': 'James', 'Role': 'Grader', 'Location': 'Washington Avenue'}])\n",
    "student_df = pd.DataFrame([{'Name': 'James', 'School': 'Business', 'Location': '1024 Billiard Avenue'},\n",
    "                           {'Name': 'Mike', 'School': 'Law', 'Location': 'Fraternity House #22'},\n",
    "                           {'Name': 'Sally', 'School': 'Engineering', 'Location': '512 Wilson Crescent'}])\n",
    "staff_df\n",
    "student_df\n",
    "pd.merge(staff_df, student_df, how='left', left_on='Name', right_on='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_df = pd.DataFrame([{'First Name': 'Kelly', 'Last Name': 'Desjardins', 'Role': 'Director of HR'},\n",
    "                         {'First Name': 'Sally', 'Last Name': 'Brooks', 'Role': 'Course liasion'},\n",
    "                         {'First Name': 'James', 'Last Name': 'Wilde', 'Role': 'Grader'}])\n",
    "student_df = pd.DataFrame([{'First Name': 'James', 'Last Name': 'Hammond', 'School': 'Business'},\n",
    "                           {'First Name': 'Mike', 'Last Name': 'Smith', 'School': 'Law'},\n",
    "                           {'First Name': 'Sally', 'Last Name': 'Brooks', 'School': 'Engineering'}])\n",
    "staff_df\n",
    "student_df\n",
    "pd.merge(staff_df, student_df, how='inner', left_on=['First Name','Last Name'], right_on=['First Name','Last Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                    'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "                    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "                    index=[0, 1, 2, 3])\n",
    "\n",
    "df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n",
    "                    'B': ['B4', 'B5', 'B6', 'B7'],\n",
    "                    'C': ['C4', 'C5', 'C6', 'C7'],\n",
    "                    'D': ['D4', 'D5', 'D6', 'D7']},\n",
    "                    index=[4, 5, 6, 7])\n",
    "\n",
    "\n",
    "df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],\n",
    "                    'B': ['B8', 'B9', 'B10', 'B11'],\n",
    "                    'C': ['C8', 'C9', 'C10', 'C11'],\n",
    "                    'D': ['D8', 'D9', 'D10', 'D11']},\n",
    "                     index=[8, 9, 10, 11])\n",
    "\n",
    "\n",
    "df1\n",
    "df2\n",
    "df3\n",
    "frames = [df1, df2, df3]\n",
    "\n",
    "result = pd.concat([df2,df1,df3])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pandas.pydata.org/pandas-docs/stable/_images/merging_concat_basic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(frames, keys=['Level1', 'Level2', 'Level3'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the columns for both dataframes are not exactly the same (but with some overlaps) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'B': ['B2', 'B3', 'B6', 'B7'],\n",
    "                    'D': ['D2', 'D3', 'D6', 'D7'],\n",
    "                    'F': ['F2', 'F3', 'F6', 'F7']},\n",
    "                    index=[2, 3, 6, 7])\n",
    "\n",
    "\n",
    "result = pd.concat([df1, df4], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pandas.pydata.org/pandas-docs/stable/_images/merging_concat_axis1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply functions over the columns\n",
    "\n",
    "apply() can apply a function along any axis of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'],\n",
    "        'year': [2012, 2012, 2013, 2014, 2014],\n",
    "        'reports': [4, 24, 31, 2, 3],\n",
    "        'coverage': [25, 94, 57, 62, 70]}\n",
    "df = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.apply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a capitalization lambda function\n",
    "capitalizer = lambda x: x.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'].apply(capitalizer)\n",
    "df['name'].apply(lambda x: x.upper())\n",
    "df.name.apply(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the capitalizer lambda function over each element in the series 'name' using map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'].map(capitalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a square function to every single cell in the whole data frame using applymap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the string variable so that applymap() can run\n",
    "#df = df.drop('name', axis=1)\n",
    "\n",
    "# Return the square of every cell in the dataframe\n",
    "df.applymap(np.square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply functions over a Dataframe\n",
    "Create a function that multiplies all non-strings by 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function called times100\n",
    "def times100(x):\n",
    "    # that, if x is a string,\n",
    "    if type(x) is str:\n",
    "        # just returns it untouched\n",
    "        return x\n",
    "    # but, if not, return it multiplied by 100\n",
    "    elif x:\n",
    "        return 100 * x\n",
    "    # and leave everything else\n",
    "    else:\n",
    "        return\n",
    "\n",
    "df.applymap(times100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scales\n",
    "\n",
    "* **Ratio Scale:**\n",
    "    * units are equally spaced\n",
    "    * mathematical operations of +-*/ are all valid\n",
    "    * e.g. height and weight\n",
    "* **Interval scale:**\n",
    "    * units are equally spaced but there are no true zero\n",
    "    * e.g. Temperature in Celcius, where 0C means is not true zero\n",
    "* **Ordinal scale:**\n",
    "    * the order of the units is important, but not evenly spaced\n",
    "    * Letter grades such as A+, A are a good example\n",
    "* **Nominal scale:**\n",
    "    * sometimes called categorical data\n",
    "    * categories of data but the categories have no order with respect to one another\n",
    "    * Teams of a sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D'],\n",
    "                  index=['excellent', 'excellent', 'excellent', 'good', 'good', 'good', 'ok', 'ok', 'ok', 'poor', 'poor'])\n",
    "df.rename(columns={0: 'Grades'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Grades'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = df['Grades'].astype('category',\n",
    "                             categories=['D', 'D+', 'C-', 'C', 'C+', 'B-', 'B', 'B+', 'A-', 'A', 'A+'],\n",
    "                             ordered=True)\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades > 'C'\n",
    "grades[grades>'C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Functionality in Pandas\n",
    "\n",
    "Following table shows the type of time-related classes pandas can handle and how to create them.\n",
    "\n",
    "| Class         |      Remarks                    |  How to create |\n",
    "|---------------|:-------------------------------:|---------------:|\n",
    "| `Timestamp`\t    |  Represents a single time stamp | `to_datetime`, `Timestamp` |\n",
    "| `DatetimeIndex` |    Index of Timestamp           | `to_datetime`, `date_range`, `DatetimeIndex`|\n",
    "| `Period`        | Represents a single time span   |    `Period`          |\n",
    "| `PeriodIndex`   | Index of a Period               |`period_range`,`PeriodIndex` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Timestamp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp('9/1/2016 10:05AM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Period('1/2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Period('2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatetimeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.Series(list('abc'), [pd.Timestamp('2016-09-01'), pd.Timestamp('2016-09-02'), pd.Timestamp('2016-09-03')])\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t1.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PeriodIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = pd.Series(list('def'), [pd.Period('2016-09'), pd.Period('2016-10'), pd.Period('2016-11')])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t2.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = ['2 June 2013', 'Aug 29, 2014', '2015-06-26', '7/12/16']\n",
    "ts3 = pd.DataFrame(np.random.randint(10, 100, (4,2)), index=d1, columns=list('ab'))\n",
    "ts3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts3.index = pd.to_datetime(ts3.index)\n",
    "ts3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = pd.to_datetime('7/12/2016 13:00:00', format ='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "date2 = pd.to_datetime('9/12/2016 14:00:00', format ='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "date2 - date1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime('4.7.12', dayfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timedeltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp('9/3/2016')-pd.Timestamp('9/1/2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp('9/2/2016 8:10AM') + pd.Timedelta('12D 3H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Dates in a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('10-01-2016', periods=9, freq='2W-SUN')\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "df = pd.DataFrame({'Count 1': 100 + np.random.randint(-5, 10, 9).cumsum(),\n",
    "                  'Count 2': 120 + np.random.randint(-5, 10, 9)}, index=dates)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.weekday_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.diff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.resample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2016-12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2016-12':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.asfreq('W', method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df.plot();"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
